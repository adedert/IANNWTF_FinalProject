{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNew7XVt9CK4H70JDmgSj4b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adedert/IANNWTF_FinalProject/blob/main/Delayed_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZT5BcYeuRLc",
        "outputId": "6e84496f-47b8-4d36-f184-50a03462aeac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376097 sha256=ded33f37020fce5935cec9f85c49a73b3428d6ca1a34f7b5c18de3673a1ba095\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "metadata": {
        "id": "NADQoop1ub7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10053bbe-8e96-478e-b604-fff512b8f293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayBuffer():\n",
        "  def __init__(self, max_size, env_name, parallel_games, steps_per_sample):\n",
        "    self.max_size = max_size\n",
        "    self.parallel_games = parallel_games\n",
        "    self.steps_per_sample = steps_per_sample\n",
        "    self.envs = gym.make_vec(env_name, num_envs=self.parallel_games)\n",
        "    self.current_states, _ = self.envs.reset()\n",
        "    self.num_possible_actions = gym.make(env_name).action_space.n\n",
        "    self.data = []\n",
        "\n",
        "  def fill_buffer(self, dqn, epsilon):\n",
        "    states_list = []\n",
        "    actions_list = []\n",
        "    rewards_list = []\n",
        "    terminateds_list = []\n",
        "    next_states_list = []\n",
        "\n",
        "    for i in range(self.steps_per_sample):\n",
        "      actions = self.sample_from_policy(dqn, epsilon)\n",
        "      next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
        "      states_list.append(self.current_states)\n",
        "      actions_list.append(actions)\n",
        "      rewards_list.append(rewards)\n",
        "      terminateds_list.append(terminateds)\n",
        "      next_states_list.append(next_states)\n",
        "      self.current_states = next_states\n",
        "\n",
        "    def data_generator():\n",
        "      for states_batch, actions_batch, rewards_batch, terminateds_batch, next_states_batch in zip(states_list, actions_list, rewards_list, terminateds_list, next_states_list):\n",
        "          for i in range(self.parallel_games):\n",
        "              state = states_batch[i,:]\n",
        "              action = actions_batch[i]\n",
        "              reward = rewards_batch[i]\n",
        "              terminated = terminateds_batch[i]\n",
        "              next_state = next_states_batch[i,:]\n",
        "              yield(state, action, reward, next_state, terminated)\n",
        "\n",
        "    dataset_tensor_specs = (tf.TensorSpec(shape=(8,), dtype=tf.float32),\n",
        "                              tf.TensorSpec(shape=(), dtype=tf.int32),\n",
        "                              tf.TensorSpec(shape=(), dtype=tf.float32),\n",
        "                              tf.TensorSpec(shape=(8,), dtype=tf.float32),\n",
        "                              tf.TensorSpec(shape=(), dtype=tf.bool))\n",
        "    new_samples_dataset = tf.data.Dataset.from_generator(data_generator, output_signature=dataset_tensor_specs)\n",
        "    new_samples_dataset = new_samples_dataset.cache().shuffle(buffer_size=self.steps_per_sample * self.parallel_games, reshuffle_each_iteration=True)\n",
        "\n",
        "    self.data.append(new_samples_dataset)\n",
        "\n",
        "    #check if buffer is full and delete data if necessary\n",
        "    datapoints_in_data = len(self.data) * self.parallel_games * self.steps_per_sample\n",
        "    #print(len(self.data))\n",
        "    if datapoints_in_data > self.max_size:\n",
        "        self.data.pop(0)\n",
        "\n",
        "  def get_data(self):\n",
        "    erp_data = tf.data.Dataset.sample_from_datasets(self.data, weights=[1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset = False)\n",
        "    return erp_data\n",
        "\n",
        "  def sample_from_policy(self, dqn, epsilon):\n",
        "    q_values = dqn(self.current_states)\n",
        "    greedy_actions = tf.argmax(q_values, axis=1)\n",
        "    #sample random action\n",
        "    random_actions = tf.random.uniform(shape=(self.parallel_games,), minval=0, maxval=self.num_possible_actions, dtype=tf.int64)\n",
        "    sample_epsilon = tf.random.uniform(shape=(self.parallel_games,), minval=0, maxval=1, dtype=tf.float32) > epsilon\n",
        "    actions = tf.where(sample_epsilon, greedy_actions, random_actions).numpy()\n",
        "    return actions"
      ],
      "metadata": {
        "id": "ODgW-XZqulPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(tf.keras.Model):\n",
        "  def __init__(self, num_actions):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "    self.fc2 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "    self.out = tf.keras.layers.Dense(num_actions)\n",
        "\n",
        "    self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\")]\n",
        "\n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "      return self.metrics_list\n",
        "\n",
        "  def reset_metrics(self):\n",
        "      for metric in self.metrics:\n",
        "          metric.reset_state()\n",
        "\n",
        "  def call(self, input):\n",
        "    x = self.fc1(input)\n",
        "    x = self.fc2(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Q9ThC72-_Cao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dqn_agent():\n",
        "  def __init__(self, dqn_network, target_network, erp, env_name, gamma, tau):\n",
        "    self.dqn_network = dqn_network\n",
        "    self.target_network = target_network\n",
        "    self.erp = erp\n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.epsilon = 1\n",
        "    self.env_name = env_name\n",
        "\n",
        "  def polyak_average(self, polyak_factor):\n",
        "    dqn_network_weights = self.dqn_network.get_weights()\n",
        "    target_network_weights = self.target_network.get_weights()\n",
        "    averaged_weights = []\n",
        "    for source_weight, target_weight in zip(dqn_network_weights, target_network_weights):\n",
        "        fraction_kept_weights = polyak_factor * target_weight\n",
        "        fraction_updated_weights = (1-polyak_factor) * source_weight\n",
        "        averaged_weight = fraction_kept_weights + fraction_updated_weights\n",
        "        averaged_weights.append(averaged_weight)\n",
        "    self.target_network.set_weights(averaged_weights)\n",
        "\n",
        "  def train_step(self, dataset, gamma, num_training_steps, batch_size=128):\n",
        "\n",
        "    @tf.function\n",
        "    def compute_gradients(q_target, states, actions):\n",
        "      #compute loss and apply gradients\n",
        "      tmp1 = []\n",
        "      tmp2 = []\n",
        "      with tf.GradientTape() as tape:\n",
        "          q_pred = self.dqn_network(states) # shape (batch_size, num_actions)\n",
        "          tmp1.append(q_pred)\n",
        "          q_pred = tf.gather(q_pred, actions, batch_dims=1)\n",
        "          tmp2.append(q_pred)\n",
        "          loss = tf.reduce_mean(tf.square(q_pred - q_target))\n",
        "      gradients = tape.gradient(loss, self.dqn_network.trainable_variables)\n",
        "      self.dqn_network.optimizer.apply_gradients(zip(gradients, self.dqn_network.trainable_variables))\n",
        "      return loss, tmp1, tmp2\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(3)\n",
        "\n",
        "    losses, q_values = [], []\n",
        "    for i, values in enumerate(dataset):\n",
        "      state, action, reward, next_state, terminated = values\n",
        "      q_vals = self.target_network(next_state)\n",
        "      q_values.append(q_vals.numpy())\n",
        "      max_q_values = tf.reduce_max(q_vals, axis=1)\n",
        "      check_terminateds = tf.where(terminated, tf.zeros_like(max_q_values, dtype=tf.float32), tf.ones_like(max_q_values, dtype=tf.float32))\n",
        "      q_target = reward + (gamma*max_q_values*check_terminateds)\n",
        "      loss, tmp1, tmp2 = compute_gradients(q_target, states=state, actions=action)\n",
        "      #print(f'state: {state.shape}, q vals: {q_vals.shape}, max q values: {max_q_values.shape}')\n",
        "      #print(f'shape of q_target: {q_target}, before: {tmp1}, after: {tmp2}')\n",
        "      losses.append(loss)\n",
        "      if i >= num_training_steps:\n",
        "          break\n",
        "    return np.mean(losses), np.mean(q_values)\n",
        "\n",
        "  def test_step(self, env_name, num_test_envs, gamma):\n",
        "    envs = gym.make_vec(env_name, num_envs=num_test_envs)\n",
        "    num_possible_actions = envs.single_action_space.n\n",
        "    states, _ = envs.reset()\n",
        "    done = False\n",
        "    timestep = 0\n",
        "    #track reward and which envs are finished\n",
        "    score = np.zeros(num_test_envs)\n",
        "    episodes_finished = np.zeros(num_test_envs, dtype=bool)\n",
        "    test_steps = 0\n",
        "    while not done:\n",
        "        q_values = self.dqn_network(states)\n",
        "        actions = tf.argmax(q_values, axis=1) # tensor of type tf.int64, shape (num_parallel_tests,)\n",
        "        states, rewards, terminateds, _, _ = envs.step(actions.numpy())\n",
        "        # compute pointwise or between episodes_finished and terminateds\n",
        "        episodes_finished = np.logical_or(episodes_finished, terminateds)\n",
        "        #create vector to only add rewards from runnning games\n",
        "        unfinished_games = np.where(episodes_finished == 1, 0, 1)\n",
        "        #print(unfinished_games)\n",
        "        score += rewards*unfinished_games\n",
        "        #returns += ((gamma**timestep)*rewards)*(np.logical_not(episodes_finished).astype(np.float32))\n",
        "        timestep += 1\n",
        "        # done if all episodes are finished\n",
        "        done = np.all(episodes_finished)\n",
        "        test_steps += 1\n",
        "        if test_steps % 100 == 0:\n",
        "           print(f\"test_steps: {test_steps} {np.sum(episodes_finished)/num_test_envs} {terminateds.shape} {episodes_finished.shape}\")\n",
        "    return np.mean(score)\n",
        "\n",
        "  def train(self, epochs):\n",
        "\n",
        "    self.polyak_average(polyak_factor=0.0)\n",
        "    rewards = []\n",
        "    #prefill buffer\n",
        "    for _ in range(20):\n",
        "      self.erp.fill_buffer(self.dqn_network, self.epsilon)\n",
        "\n",
        "    for step in range(epochs):\n",
        "      self.erp.fill_buffer(self.dqn_network, epsilon=self.epsilon)\n",
        "      data = self.erp.get_data()\n",
        "      avg_loss, avg_q_values = self.train_step(dataset=data, gamma=self.gamma, num_training_steps=4)\n",
        "      self.epsilon = max(self.epsilon*0.992, 0.05)\n",
        "      self.polyak_average(self.tau)\n",
        "      if (step+1) % 20 == 0:\n",
        "        print(f\"epoch: {step+1}, avg_loss: {avg_loss}, epsilon: {self.epsilon}\")\n",
        "      if (step+1) % 100 == 0:\n",
        "        self.dqn_network.save_weights('/content/drive/MyDrive/Lunar Lander/checkpoints/dqn_checkpoint')\n",
        "        self.target_network.save_weights('/content/drive/MyDrive/Lunar Lander/checkpoints/target_checkpoint')\n",
        "        avg_reward = self.test_step(env_name=\"LunarLander-v2\", num_test_envs=64, gamma=self.gamma)\n",
        "        rewards.append(avg_reward)\n",
        "        with open('/content/drive/MyDrive/Lunar Lander/dqn_rewards.pkl', 'wb') as f:  # open a text file\n",
        "          pickle.dump(rewards, f)\n",
        "        f.close()\n",
        "        print(f\"saved model, avg reward on test step: {avg_reward}\")\n"
      ],
      "metadata": {
        "id": "7rqdkgOf_Nlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_network = DQN(num_actions=4)\n",
        "dqn_network(tf.random.uniform(shape=(1,8)))\n",
        "target_network = DQN(num_actions=4)\n",
        "target_network(tf.random.uniform(shape=(1,8)))\n",
        "erp = ExperienceReplayBuffer(max_size=50000, env_name=\"LunarLander-v2\", parallel_games=64, steps_per_sample=5)\n",
        "\n",
        "basic_agent = dqn_agent(dqn_network=dqn_network, target_network=target_network, erp=erp, env_name=\"LunarLander-v2\", gamma=0.99, tau=0.99)"
      ],
      "metadata": {
        "id": "2Dt-dcHCHWOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN(num_actions=4)\n",
        "model(tf.random.uniform(shape=(1,8)))\n",
        "model.set_weights(dqn_network.get_weights())"
      ],
      "metadata": {
        "id": "4CccKAtvVu4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.test_step(\"LunarLander-v2\", 64, 0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdEpF-fwW0HI",
        "outputId": "f1ac38d5-5493-4f1a-8b34-281d198a3bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_steps: 100 0.09375 (64,) (64,)\n",
            "test_steps: 200 0.8125 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-430.1063479820993"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basic_agent.train(epochs=1000)"
      ],
      "metadata": {
        "id": "dX04WQH-KX01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8106040-4661-49a3-ca56-76505e24dba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 20, avg_loss: 37.61334991455078, epsilon: 0.8515956670851491\n",
            "epoch: 40, avg_loss: 72.8701400756836, epsilon: 0.7252151801981999\n",
            "epoch: 60, avg_loss: 84.17674255371094, epsilon: 0.6175901051611625\n",
            "epoch: 80, avg_loss: 42.7049674987793, epsilon: 0.5259370575899076\n",
            "epoch: 100, avg_loss: 74.7244644165039, epsilon: 0.447885719403078\n",
            "test_steps: 100 0.0 (64,) (64,)\n",
            "test_steps: 200 0.421875 (64,) (64,)\n",
            "test_steps: 300 0.703125 (64,) (64,)\n",
            "test_steps: 400 0.765625 (64,) (64,)\n",
            "test_steps: 500 0.765625 (64,) (64,)\n",
            "test_steps: 600 0.796875 (64,) (64,)\n",
            "test_steps: 700 0.796875 (64,) (64,)\n",
            "test_steps: 800 0.796875 (64,) (64,)\n",
            "test_steps: 900 0.796875 (64,) (64,)\n",
            "test_steps: 1000 0.796875 (64,) (64,)\n",
            "test_steps: 1100 0.796875 (64,) (64,)\n",
            "test_steps: 1200 0.921875 (64,) (64,)\n",
            "test_steps: 1300 0.9375 (64,) (64,)\n",
            "test_steps: 1400 0.953125 (64,) (64,)\n",
            "test_steps: 1500 0.953125 (64,) (64,)\n",
            "test_steps: 1600 0.953125 (64,) (64,)\n",
            "test_steps: 1700 0.96875 (64,) (64,)\n",
            "test_steps: 1800 0.96875 (64,) (64,)\n",
            "test_steps: 1900 0.96875 (64,) (64,)\n",
            "test_steps: 2000 0.984375 (64,) (64,)\n",
            "test_steps: 2100 0.984375 (64,) (64,)\n",
            "saved model, avg reward on test step: -195.77146127084683\n",
            "epoch: 120, avg_loss: 14.362436294555664, epsilon: 0.3814175379929761\n",
            "epoch: 140, avg_loss: 74.30728149414062, epsilon: 0.3248135227051037\n",
            "epoch: 160, avg_loss: 65.17332458496094, epsilon: 0.27660978854633006\n",
            "epoch: 180, avg_loss: 16.810335159301758, epsilon: 0.23555969739939403\n",
            "epoch: 200, avg_loss: 27.035614013671875, epsilon: 0.2006016176452128\n",
            "test_steps: 100 0.046875 (64,) (64,)\n",
            "test_steps: 200 0.453125 (64,) (64,)\n",
            "test_steps: 300 0.671875 (64,) (64,)\n",
            "test_steps: 400 0.6875 (64,) (64,)\n",
            "test_steps: 500 0.703125 (64,) (64,)\n",
            "test_steps: 600 0.703125 (64,) (64,)\n",
            "test_steps: 700 0.703125 (64,) (64,)\n",
            "test_steps: 800 0.703125 (64,) (64,)\n",
            "test_steps: 900 0.703125 (64,) (64,)\n",
            "test_steps: 1000 0.703125 (64,) (64,)\n",
            "test_steps: 1100 0.703125 (64,) (64,)\n",
            "test_steps: 1200 0.84375 (64,) (64,)\n",
            "test_steps: 1300 0.875 (64,) (64,)\n",
            "test_steps: 1400 0.890625 (64,) (64,)\n",
            "test_steps: 1500 0.890625 (64,) (64,)\n",
            "test_steps: 1600 0.890625 (64,) (64,)\n",
            "test_steps: 1700 0.890625 (64,) (64,)\n",
            "test_steps: 1800 0.890625 (64,) (64,)\n",
            "test_steps: 1900 0.890625 (64,) (64,)\n",
            "test_steps: 2000 0.890625 (64,) (64,)\n",
            "test_steps: 2100 0.890625 (64,) (64,)\n",
            "test_steps: 2200 0.9375 (64,) (64,)\n",
            "test_steps: 2300 0.96875 (64,) (64,)\n",
            "test_steps: 2400 0.96875 (64,) (64,)\n",
            "test_steps: 2500 0.96875 (64,) (64,)\n",
            "test_steps: 2600 0.96875 (64,) (64,)\n",
            "test_steps: 2700 0.96875 (64,) (64,)\n",
            "test_steps: 2800 0.96875 (64,) (64,)\n",
            "test_steps: 2900 0.96875 (64,) (64,)\n",
            "test_steps: 3000 0.96875 (64,) (64,)\n",
            "test_steps: 3100 0.96875 (64,) (64,)\n",
            "test_steps: 3200 0.96875 (64,) (64,)\n",
            "test_steps: 3300 0.96875 (64,) (64,)\n",
            "test_steps: 3400 0.96875 (64,) (64,)\n",
            "test_steps: 3500 0.96875 (64,) (64,)\n",
            "test_steps: 3600 0.96875 (64,) (64,)\n",
            "test_steps: 3700 0.96875 (64,) (64,)\n",
            "test_steps: 3800 0.96875 (64,) (64,)\n",
            "test_steps: 3900 0.96875 (64,) (64,)\n",
            "test_steps: 4000 0.96875 (64,) (64,)\n",
            "test_steps: 4100 0.984375 (64,) (64,)\n",
            "test_steps: 4200 0.984375 (64,) (64,)\n",
            "saved model, avg reward on test step: -476.0426798260241\n",
            "epoch: 220, avg_loss: 21.29762840270996, epsilon: 0.17083146839693503\n",
            "epoch: 240, avg_loss: 4.790615558624268, epsilon: 0.14547933828862347\n",
            "epoch: 260, avg_loss: 35.67173385620117, epsilon: 0.12388957413700638\n",
            "epoch: 280, avg_loss: 14.031393051147461, epsilon: 0.10550382453209899\n",
            "epoch: 300, avg_loss: 15.466137886047363, epsilon: 0.08984659983244736\n",
            "test_steps: 100 0.0 (64,) (64,)\n",
            "test_steps: 200 0.578125 (64,) (64,)\n",
            "test_steps: 300 0.90625 (64,) (64,)\n",
            "saved model, avg reward on test step: -96.27303212637386\n",
            "epoch: 320, avg_loss: 24.207393646240234, epsilon: 0.07651297511964544\n",
            "epoch: 340, avg_loss: 9.561429023742676, epsilon: 0.06515811808768388\n",
            "epoch: 360, avg_loss: 5.498105525970459, epsilon: 0.05548837103889405\n",
            "epoch: 380, avg_loss: 10.810382843017578, epsilon: 0.05\n",
            "epoch: 400, avg_loss: 20.84537696838379, epsilon: 0.05\n",
            "test_steps: 100 0.0 (64,) (64,)\n",
            "test_steps: 200 0.0 (64,) (64,)\n",
            "test_steps: 300 0.03125 (64,) (64,)\n",
            "test_steps: 400 0.28125 (64,) (64,)\n",
            "test_steps: 500 0.609375 (64,) (64,)\n",
            "test_steps: 600 0.75 (64,) (64,)\n",
            "test_steps: 700 0.859375 (64,) (64,)\n",
            "test_steps: 800 0.90625 (64,) (64,)\n",
            "test_steps: 900 0.96875 (64,) (64,)\n",
            "saved model, avg reward on test step: -13.83425805975276\n",
            "epoch: 420, avg_loss: 31.29019546508789, epsilon: 0.05\n",
            "epoch: 440, avg_loss: 18.253150939941406, epsilon: 0.05\n",
            "epoch: 460, avg_loss: 55.335693359375, epsilon: 0.05\n",
            "epoch: 480, avg_loss: 13.006213188171387, epsilon: 0.05\n",
            "epoch: 500, avg_loss: 24.887487411499023, epsilon: 0.05\n",
            "test_steps: 100 0.015625 (64,) (64,)\n",
            "test_steps: 200 0.828125 (64,) (64,)\n",
            "test_steps: 300 0.953125 (64,) (64,)\n",
            "saved model, avg reward on test step: -274.1101318061849\n",
            "epoch: 520, avg_loss: 16.102100372314453, epsilon: 0.05\n",
            "epoch: 540, avg_loss: 3.0266854763031006, epsilon: 0.05\n",
            "epoch: 560, avg_loss: 14.169212341308594, epsilon: 0.05\n",
            "epoch: 580, avg_loss: 16.821460723876953, epsilon: 0.05\n",
            "epoch: 600, avg_loss: 5.6515913009643555, epsilon: 0.05\n",
            "test_steps: 100 0.0 (64,) (64,)\n",
            "test_steps: 200 0.0 (64,) (64,)\n",
            "test_steps: 300 0.09375 (64,) (64,)\n",
            "test_steps: 400 0.375 (64,) (64,)\n",
            "test_steps: 500 0.59375 (64,) (64,)\n",
            "test_steps: 600 0.6875 (64,) (64,)\n",
            "test_steps: 700 0.765625 (64,) (64,)\n",
            "test_steps: 800 0.84375 (64,) (64,)\n",
            "test_steps: 900 0.953125 (64,) (64,)\n",
            "test_steps: 1000 0.953125 (64,) (64,)\n",
            "test_steps: 1100 0.953125 (64,) (64,)\n",
            "test_steps: 1200 0.953125 (64,) (64,)\n",
            "test_steps: 1300 0.953125 (64,) (64,)\n",
            "test_steps: 1400 0.96875 (64,) (64,)\n",
            "test_steps: 1500 0.984375 (64,) (64,)\n",
            "test_steps: 1600 0.984375 (64,) (64,)\n",
            "test_steps: 1700 0.984375 (64,) (64,)\n",
            "saved model, avg reward on test step: 9.607301745740463\n",
            "epoch: 620, avg_loss: 12.577377319335938, epsilon: 0.05\n",
            "epoch: 640, avg_loss: 14.219446182250977, epsilon: 0.05\n",
            "epoch: 660, avg_loss: 25.340351104736328, epsilon: 0.05\n",
            "epoch: 680, avg_loss: 15.803899765014648, epsilon: 0.05\n",
            "epoch: 700, avg_loss: 13.103475570678711, epsilon: 0.05\n",
            "test_steps: 100 0.0 (64,) (64,)\n",
            "test_steps: 200 0.015625 (64,) (64,)\n",
            "test_steps: 300 0.234375 (64,) (64,)\n",
            "test_steps: 400 0.453125 (64,) (64,)\n",
            "test_steps: 500 0.515625 (64,) (64,)\n",
            "test_steps: 600 0.609375 (64,) (64,)\n",
            "test_steps: 700 0.671875 (64,) (64,)\n",
            "test_steps: 800 0.703125 (64,) (64,)\n",
            "test_steps: 900 0.734375 (64,) (64,)\n",
            "test_steps: 1000 0.78125 (64,) (64,)\n",
            "test_steps: 1100 0.78125 (64,) (64,)\n",
            "test_steps: 1200 0.78125 (64,) (64,)\n",
            "test_steps: 1300 0.875 (64,) (64,)\n",
            "test_steps: 1400 0.9375 (64,) (64,)\n",
            "test_steps: 1500 0.953125 (64,) (64,)\n",
            "test_steps: 1600 0.953125 (64,) (64,)\n",
            "test_steps: 1700 0.96875 (64,) (64,)\n",
            "test_steps: 1800 0.96875 (64,) (64,)\n",
            "test_steps: 1900 0.96875 (64,) (64,)\n",
            "test_steps: 2000 0.96875 (64,) (64,)\n",
            "test_steps: 2100 0.96875 (64,) (64,)\n",
            "test_steps: 2200 0.96875 (64,) (64,)\n",
            "test_steps: 2300 0.96875 (64,) (64,)\n",
            "test_steps: 2400 0.96875 (64,) (64,)\n",
            "test_steps: 2500 0.984375 (64,) (64,)\n",
            "saved model, avg reward on test step: 40.94161275485905\n",
            "epoch: 720, avg_loss: 3.016317129135132, epsilon: 0.05\n",
            "epoch: 740, avg_loss: 4.470318794250488, epsilon: 0.05\n",
            "epoch: 760, avg_loss: 7.264151096343994, epsilon: 0.05\n",
            "epoch: 780, avg_loss: 23.64535140991211, epsilon: 0.05\n",
            "epoch: 800, avg_loss: 38.428672790527344, epsilon: 0.05\n",
            "test_steps: 100 0.0 (64,) (64,)\n",
            "test_steps: 200 0.015625 (64,) (64,)\n",
            "test_steps: 300 0.171875 (64,) (64,)\n",
            "test_steps: 400 0.28125 (64,) (64,)\n",
            "test_steps: 500 0.375 (64,) (64,)\n",
            "test_steps: 600 0.46875 (64,) (64,)\n",
            "test_steps: 700 0.5 (64,) (64,)\n",
            "test_steps: 800 0.515625 (64,) (64,)\n",
            "test_steps: 900 0.515625 (64,) (64,)\n",
            "test_steps: 1000 0.53125 (64,) (64,)\n",
            "test_steps: 1100 0.53125 (64,) (64,)\n",
            "test_steps: 1200 0.546875 (64,) (64,)\n",
            "test_steps: 1300 0.640625 (64,) (64,)\n",
            "test_steps: 1400 0.75 (64,) (64,)\n",
            "test_steps: 1500 0.796875 (64,) (64,)\n",
            "test_steps: 1600 0.8125 (64,) (64,)\n",
            "test_steps: 1700 0.828125 (64,) (64,)\n",
            "test_steps: 1800 0.828125 (64,) (64,)\n",
            "test_steps: 1900 0.828125 (64,) (64,)\n",
            "test_steps: 2000 0.84375 (64,) (64,)\n",
            "test_steps: 2100 0.84375 (64,) (64,)\n",
            "test_steps: 2200 0.84375 (64,) (64,)\n",
            "test_steps: 2300 0.875 (64,) (64,)\n",
            "test_steps: 2400 0.875 (64,) (64,)\n",
            "test_steps: 2500 0.890625 (64,) (64,)\n",
            "test_steps: 2600 0.890625 (64,) (64,)\n",
            "test_steps: 2700 0.890625 (64,) (64,)\n",
            "test_steps: 2800 0.890625 (64,) (64,)\n",
            "test_steps: 2900 0.890625 (64,) (64,)\n",
            "test_steps: 3000 0.890625 (64,) (64,)\n",
            "test_steps: 3100 0.890625 (64,) (64,)\n",
            "test_steps: 3200 0.890625 (64,) (64,)\n",
            "test_steps: 3300 0.90625 (64,) (64,)\n",
            "test_steps: 3400 0.90625 (64,) (64,)\n",
            "test_steps: 3500 0.921875 (64,) (64,)\n",
            "test_steps: 3600 0.953125 (64,) (64,)\n",
            "test_steps: 3700 0.953125 (64,) (64,)\n",
            "test_steps: 3800 0.953125 (64,) (64,)\n",
            "test_steps: 3900 0.96875 (64,) (64,)\n",
            "test_steps: 4000 0.96875 (64,) (64,)\n",
            "test_steps: 4100 0.96875 (64,) (64,)\n",
            "test_steps: 4200 0.96875 (64,) (64,)\n",
            "test_steps: 4300 0.96875 (64,) (64,)\n",
            "test_steps: 4400 0.984375 (64,) (64,)\n",
            "test_steps: 4500 0.984375 (64,) (64,)\n",
            "test_steps: 4600 0.984375 (64,) (64,)\n",
            "test_steps: 4700 0.984375 (64,) (64,)\n",
            "test_steps: 4800 0.984375 (64,) (64,)\n",
            "test_steps: 4900 0.984375 (64,) (64,)\n",
            "saved model, avg reward on test step: -7.431830060774309\n",
            "epoch: 820, avg_loss: 18.149593353271484, epsilon: 0.05\n",
            "epoch: 840, avg_loss: 8.074779510498047, epsilon: 0.05\n",
            "epoch: 860, avg_loss: 44.75106430053711, epsilon: 0.05\n",
            "epoch: 880, avg_loss: 14.105290412902832, epsilon: 0.05\n",
            "epoch: 900, avg_loss: 25.891010284423828, epsilon: 0.05\n",
            "test_steps: 100 0.0 (64,) (64,)\n",
            "test_steps: 200 0.03125 (64,) (64,)\n",
            "test_steps: 300 0.234375 (64,) (64,)\n",
            "test_steps: 400 0.453125 (64,) (64,)\n",
            "test_steps: 500 0.578125 (64,) (64,)\n",
            "test_steps: 600 0.578125 (64,) (64,)\n",
            "test_steps: 700 0.625 (64,) (64,)\n",
            "test_steps: 800 0.65625 (64,) (64,)\n",
            "test_steps: 900 0.671875 (64,) (64,)\n",
            "test_steps: 1000 0.671875 (64,) (64,)\n",
            "test_steps: 1100 0.671875 (64,) (64,)\n",
            "test_steps: 1200 0.671875 (64,) (64,)\n",
            "test_steps: 1300 0.734375 (64,) (64,)\n",
            "test_steps: 1400 0.8125 (64,) (64,)\n",
            "test_steps: 1500 0.859375 (64,) (64,)\n",
            "test_steps: 1600 0.859375 (64,) (64,)\n",
            "test_steps: 1700 0.875 (64,) (64,)\n",
            "test_steps: 1800 0.875 (64,) (64,)\n",
            "test_steps: 1900 0.875 (64,) (64,)\n",
            "test_steps: 2000 0.875 (64,) (64,)\n",
            "test_steps: 2100 0.875 (64,) (64,)\n",
            "test_steps: 2200 0.890625 (64,) (64,)\n",
            "test_steps: 2300 0.90625 (64,) (64,)\n",
            "test_steps: 2400 0.9375 (64,) (64,)\n",
            "test_steps: 2500 0.953125 (64,) (64,)\n",
            "test_steps: 2600 0.953125 (64,) (64,)\n",
            "test_steps: 2700 0.953125 (64,) (64,)\n",
            "test_steps: 2800 0.953125 (64,) (64,)\n",
            "test_steps: 2900 0.953125 (64,) (64,)\n",
            "test_steps: 3000 0.96875 (64,) (64,)\n",
            "test_steps: 3100 0.96875 (64,) (64,)\n",
            "test_steps: 3200 0.96875 (64,) (64,)\n",
            "test_steps: 3300 0.984375 (64,) (64,)\n",
            "saved model, avg reward on test step: -17.51535170660712\n",
            "epoch: 920, avg_loss: 23.82796287536621, epsilon: 0.05\n",
            "epoch: 940, avg_loss: 3.449082851409912, epsilon: 0.05\n",
            "epoch: 960, avg_loss: 2.3125393390655518, epsilon: 0.05\n",
            "epoch: 980, avg_loss: 3.3748245239257812, epsilon: 0.05\n",
            "epoch: 1000, avg_loss: 23.69951820373535, epsilon: 0.05\n",
            "test_steps: 100 0.0 (64,) (64,)\n",
            "test_steps: 200 0.0 (64,) (64,)\n",
            "test_steps: 300 0.265625 (64,) (64,)\n",
            "test_steps: 400 0.421875 (64,) (64,)\n",
            "test_steps: 500 0.46875 (64,) (64,)\n",
            "test_steps: 600 0.5 (64,) (64,)\n",
            "test_steps: 700 0.53125 (64,) (64,)\n",
            "test_steps: 800 0.546875 (64,) (64,)\n",
            "test_steps: 900 0.546875 (64,) (64,)\n",
            "test_steps: 1000 0.5625 (64,) (64,)\n",
            "test_steps: 1100 0.5625 (64,) (64,)\n",
            "test_steps: 1200 0.5625 (64,) (64,)\n",
            "test_steps: 1300 0.625 (64,) (64,)\n",
            "test_steps: 1400 0.71875 (64,) (64,)\n",
            "test_steps: 1500 0.75 (64,) (64,)\n",
            "test_steps: 1600 0.75 (64,) (64,)\n",
            "test_steps: 1700 0.765625 (64,) (64,)\n",
            "test_steps: 1800 0.765625 (64,) (64,)\n",
            "test_steps: 1900 0.796875 (64,) (64,)\n",
            "test_steps: 2000 0.796875 (64,) (64,)\n",
            "test_steps: 2100 0.796875 (64,) (64,)\n",
            "test_steps: 2200 0.796875 (64,) (64,)\n",
            "test_steps: 2300 0.828125 (64,) (64,)\n",
            "test_steps: 2400 0.875 (64,) (64,)\n",
            "test_steps: 2500 0.890625 (64,) (64,)\n",
            "test_steps: 2600 0.90625 (64,) (64,)\n",
            "test_steps: 2700 0.90625 (64,) (64,)\n",
            "test_steps: 2800 0.921875 (64,) (64,)\n",
            "test_steps: 2900 0.921875 (64,) (64,)\n",
            "test_steps: 3000 0.921875 (64,) (64,)\n",
            "test_steps: 3100 0.921875 (64,) (64,)\n",
            "test_steps: 3200 0.921875 (64,) (64,)\n",
            "test_steps: 3300 0.9375 (64,) (64,)\n",
            "test_steps: 3400 0.9375 (64,) (64,)\n",
            "test_steps: 3500 0.9375 (64,) (64,)\n",
            "test_steps: 3600 0.9375 (64,) (64,)\n",
            "test_steps: 3700 0.9375 (64,) (64,)\n",
            "test_steps: 3800 0.9375 (64,) (64,)\n",
            "test_steps: 3900 0.9375 (64,) (64,)\n",
            "test_steps: 4000 0.9375 (64,) (64,)\n",
            "test_steps: 4100 0.9375 (64,) (64,)\n",
            "test_steps: 4200 0.9375 (64,) (64,)\n",
            "test_steps: 4300 0.953125 (64,) (64,)\n",
            "test_steps: 4400 0.953125 (64,) (64,)\n",
            "test_steps: 4500 0.953125 (64,) (64,)\n",
            "test_steps: 4600 0.953125 (64,) (64,)\n",
            "test_steps: 4700 0.953125 (64,) (64,)\n",
            "test_steps: 4800 0.953125 (64,) (64,)\n",
            "test_steps: 4900 0.953125 (64,) (64,)\n",
            "test_steps: 5000 0.953125 (64,) (64,)\n",
            "test_steps: 5100 0.953125 (64,) (64,)\n",
            "test_steps: 5200 0.953125 (64,) (64,)\n",
            "test_steps: 5300 0.984375 (64,) (64,)\n",
            "test_steps: 5400 0.984375 (64,) (64,)\n",
            "saved model, avg reward on test step: 14.380062646802717\n"
          ]
        }
      ]
    }
  ]
}